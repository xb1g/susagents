{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase, basic_auth\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to neo4j\n"
     ]
    }
   ],
   "source": [
    "#load the environment variables\n",
    "dotenv_path = Path('/Users/bunyasit/dev/sustaingraph/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)  # This line brings all environment variables from .env into os.environ\n",
    "\n",
    "# Get variables\n",
    "SUSTAINGRAPH_URI = os.getenv('SUSTAINGRAPH_URI')\n",
    "SUSTAINGRAPH_USER = os.getenv('SUSTAINGRAPH_USER')\n",
    "SUSTAINGRAPH_PASSWORD = os.getenv('SUSTAINGRAPH_PASSWORD')\n",
    "database_name = os.getenv('DATABASE_NAME')\n",
    "\n",
    "# Connect to database\n",
    "driver = GraphDatabase.driver(SUSTAINGRAPH_URI, auth=(SUSTAINGRAPH_USER, SUSTAINGRAPH_PASSWORD))\n",
    "\n",
    "# Verify connectivity\n",
    "with driver.session(database=database_name) as session:\n",
    "    print(session.run(\"RETURN 'Connected to ' + $db\", db=database_name).single()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints\n",
    "\n",
    "Before importing any data we add a Unique node property constraint on the properties _M49code_ , _ISOalpha2code_, _ISOalpha3code_ , _EUcode_ of _GeoArea_ (and therefore an index too) for data integrity and better query performance. A type constrant is also added on properties _name_ and the aforementioned codes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraint(tx,statement):\n",
    "    tx.run(statement)\n",
    "\n",
    "constraints = [\n",
    "    \"\"\"CREATE CONSTRAINT geoArea_M49code IF NOT EXISTS FOR (ga:GeoArea) REQUIRE ga.M49code IS UNIQUE\"\"\",\n",
    "    \"\"\"CREATE CONSTRAINT geoArea_ISO2code IF NOT EXISTS FOR (ga:GeoArea) REQUIRE ga.ISOalpha2code IS UNIQUE\"\"\",\n",
    "    \"\"\"CREATE CONSTRAINT geoArea_ISO3code IF NOT EXISTS FOR (ga:GeoArea) REQUIRE ga.ISOalpha3code IS UNIQUE\"\"\",\n",
    "    \"\"\"CREATE CONSTRAINT geoArea_EUcode IF NOT EXISTS FOR (ga:GeoArea) REQUIRE ga.EUcode IS UNIQUE\"\"\",\n",
    "    \"\"\"CREATE CONSTRAINT geoArea_M49code_type IF NOT EXISTS FOR (ga:GeoArea) REQUIRE ga.M49code IS :: STRING\"\"\",\n",
    "    \"\"\"CREATE CONSTRAINT geoArea_ISO2code_type IF NOT EXISTS FOR (ga:GeoArea) REQUIRE ga.ISOalpha2code IS :: STRING\"\"\",\n",
    "    \"\"\"CREATE CONSTRAINT geoArea_ISO3code_type IF NOT EXISTS FOR (ga:GeoArea) REQUIRE ga.ISOalpha3code IS :: STRING\"\"\",\n",
    "    \"\"\"CREATE CONSTRAINT geoArea_EUcode_type IF NOT EXISTS FOR (ga:GeoArea) REQUIRE ga.EUcode IS :: STRING\"\"\",\n",
    "    \"\"\"CREATE CONSTRAINT geoArea_name_type IF NOT EXISTS FOR (ga:GeoArea) REQUIRE ga.name IS :: STRING\"\"\"\n",
    "]\n",
    "\n",
    "with driver.session(database=database_name) as session:\n",
    "    for statement_constraint in constraints:\n",
    "        session.execute_write(create_constraint, statement_constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_batch(tx,statement, params_list):\n",
    "    tx.run(statement, parameters={\"parameters\": params_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import GeoAreas to Neo4j from United Nations (Regions,SubRegions,Areas) and Eurostat (Areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [United Nations publication \"Standard Country or Area Codes for Statistical Use\"](https://unstats.un.org/unsd/methodology/m49/) offers a list of countries or areas contains the names of countries or areas in alphabetical order, their three-digit numerical codes used for statistical processing purposes by the Statistics Division of the United Nations Secretariat, and their two- and three-digit alphabetical codes assigned by the International Organization for Standardization ( ISO 3166 alpha-2 and alpha-3). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 observations: Done! (0.012616884708404542 minutes)\n"
     ]
    }
   ],
   "source": [
    "# read csv \n",
    "df = pd.read_csv('Data/1.GeoArea_UN.csv',sep=';',usecols=['Global Code', 'Global Name', 'Region Code', 'Region Name',\n",
    "       'Sub-region Code', 'Sub-region Name', 'Country or Area', 'M49 Code',\n",
    "       'ISO-alpha2 Code', 'ISO-alpha3 Code'])\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Query statement to import the data in batches of 10000 rows to avoid memory issues\n",
    "statement_geo = \"\"\"\n",
    "    UNWIND $parameters as row\n",
    "    WITH row \n",
    "    MERGE (world:GeoArea{name:row.world_name,M49:row.world_code})\n",
    "    MERGE (world)-[:HAS_REGION]->(reg:GeoArea{name:row.reg_name,M49code:row.reg_code})\n",
    "    MERGE (reg)-[:HAS_SUBREGION]->(subreg:GeoArea{name:row.sub_name,M49code:row.sub_code})\n",
    "    MERGE (subreg)-[:HAS_AREA]->(area:GeoArea{name:row.area_name,M49code:row.m49, ISOalpha2code:row.iso2, ISOalpha3code:row.iso3})\n",
    "    \"\"\" \n",
    "\n",
    "params=[]\n",
    "batch_size = 10000\n",
    "batch_i = 1\n",
    "with driver.session(database=database_name) as session:\n",
    "    for index, row in df.iterrows():\n",
    "        st = time.time()\n",
    "        params_dict = {\n",
    "            'world_name': str(row['Global Name']), \n",
    "            'world_code': str(int(row['Global Code'])),\n",
    "            'reg_name': str(row['Region Name']),\n",
    "            'reg_code':str(int(row['Region Code'])),\n",
    "            'sub_name':str(row['Sub-region Name']),\n",
    "            'sub_code': str(int(row['Sub-region Code'])),\n",
    "            'area_name': str(row['Country or Area']),\n",
    "            'm49':str(row['M49 Code']),\n",
    "            'iso2':str(row['ISO-alpha2 Code']),\n",
    "            'iso3':str(row['ISO-alpha3 Code'])\n",
    "        }\n",
    "        params.append(params_dict)\n",
    "        if index % batch_size == 0 and index > 0:\n",
    "            st = time.time()\n",
    "            session.execute_write(write_batch, params_list = params,statement = statement_geo)\n",
    "            # driver.execute_query(statement,parameters=params)\n",
    "            et = time.time()\n",
    "            # get the execution time\n",
    "            elapsed_time = et - st            \n",
    "            print('Batch {} with {} data : Done! ({} minutes)'.format(batch_i,len(params),elapsed_time/60))\n",
    "            params = []            \n",
    "            batch_i +=1\n",
    "        \n",
    "    if params:\n",
    "            st = time.time()  # Record start time for the last batch\n",
    "            session.execute_write(write_batch, params_list = params,statement = statement_geo)\n",
    "            et = time.time()\n",
    "            elapsed_time = et - st\n",
    "            print('{} observations: Done! ({} minutes)'.format(len(params), elapsed_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check cypher query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continents: 5\n",
      "SubRegions: 17\n",
      "Areas: 246\n"
     ]
    }
   ],
   "source": [
    "def check_regions(tx,statement):\n",
    "    result = tx.run(statement)\n",
    "    data = result.data()[0]\n",
    "    return data['continents'],data['subregions'], data['areas']\n",
    "\n",
    "statement_check = \"\"\" \n",
    "MATCH (ga:GeoArea)-[:HAS_REGION]->(r)-[:HAS_SUBREGION]->(sb)-[:HAS_AREA]->(a)\n",
    "RETURN COUNT(DISTINCT r) as continents,COUNT(DISTINCT sb) as subregions,COUNT(DISTINCT a) as areas\n",
    "\"\"\" \n",
    "\n",
    "with driver.session(database=database_name) as session:\n",
    "    continents, subregions, areas = session.execute_write(check_regions, statement_check)\n",
    "    print('Continents:', continents)\n",
    "    print('SubRegions:', subregions)\n",
    "    print('Areas:', areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Eurostat database has assigned to Member States of the European Union (EU) and other countries a two-letter country code, always written in capital letters, and often used as an abbreviation in statistical analyses, tables, figures or maps. In the _Data/1.GeoArea_EU.xlsx_ file there is the mapping for countries with their EUcodes based on https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Glossary:Country_codes\n",
    "\n",
    "Several country names where replaced to match the name given from the   United Nations:\n",
    "- EU_name --> UN_name \n",
    "- Moldova -> Republic of Moldova\n",
    "- Türkiye -> Turkey\n",
    "- Palestine -> State of Palestine\n",
    "- Syria -> Syrian Arab Republic\n",
    "- Hong Kong -> China, Hong Kong Special Administrative Region\n",
    "- Russia -> Russian Federation\n",
    "- South Korea -> Republic of Korea\n",
    "- United Kingdom -> United Kingdom of Great Britain and Northern Ireland\n",
    "- United States -> United States of America\n",
    "\n",
    "Kosovo and Taiwan mentioned in EU dataset are not present in the UN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries with EU codes: 70\n",
      "Countries updated with  EU codes: 70\n"
     ]
    }
   ],
   "source": [
    "eu_df = pd.read_excel('Data/1.GeoArea_EU.xlsx')\n",
    "eu_df.replace(['Moldova', 'Türkiye', 'Palestine','Syria','Hong Kong','Russia','South Korea','United Kingdom','United States'],\n",
    "                 ['Republic of Moldova', 'Turkey', 'State of Palestine','Syrian Arab Republic',\n",
    "                  'China, Hong Kong Special Administrative Region','Russian Federation','Republic of Korea',\n",
    "                  'United Kingdom of Great Britain and Northern Ireland',' United States of America'],inplace=True)\n",
    "\n",
    "eu_codes = []\n",
    "\n",
    "def write_eu(tx,statement, eu_codes):\n",
    "    result = tx.run(statement, parameters={\"eu_codes\": eu_codes})\n",
    "    total = result.data()[0]['total']\n",
    "    return total\n",
    "\n",
    "statement_eu = \"\"\"\n",
    "    UNWIND $eu_codes as country\n",
    "    WITH country \n",
    "    MATCH (area:GeoArea{name:country.area_name})\n",
    "    SET area.EUcode = country.eu_code\n",
    "    RETURN COUNT(area) as total\n",
    "    \"\"\" \n",
    "with driver.session(database=database_name) as session:\n",
    "    for index, row in eu_df.iterrows():\n",
    "        params_dict = {\n",
    "            'area_name': str(row['Name']).strip(),\n",
    "            'eu_code':str(row['Code']),\n",
    "        }\n",
    "        eu_codes.append(params_dict)\n",
    "    total_updates = session.execute_write(write_eu,statement_eu, eu_codes)\n",
    "    print(\"Countries with EU codes:\",len(eu_df['Name']) - 2) # Kosovo and Taiwan\n",
    "    print(\"Countries updated with  EU codes:\",total_updates) # Kosovo and Taiwan\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### European Union\n",
    "Create a node for the European Union where all the 27-countries belong to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries member of EU added: 27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def write_eu(tx,statement, eu_members):\n",
    "    result = tx.run(statement, parameters={\"eu_members\": eu_members})\n",
    "    total = result.data()[0]['total']\n",
    "    return total\n",
    "\n",
    "statement_eu_mem = \"\"\"\n",
    "    MERGE (eu:EuropeanUnion{description:'The European Union, abbreviated as EU, is an economic and political union of European countries with 27 member states.'})\n",
    "    WITH eu\n",
    "    UNWIND $eu_members as code\n",
    "    MATCH (a:GeoArea{EUcode:code})\n",
    "    MERGE (a)-[b:BELONGS_TO]->(eu)\n",
    "    RETURN COUNT(b) as total\n",
    "    \"\"\" \n",
    "eu_members = []\n",
    "with driver.session(database=database_name) as session:\n",
    "    for index, row in eu_df.iterrows():\n",
    "        if row['Classification'] == 'European Union':\n",
    "            eu_members.append(row['Code'])\n",
    "    total_updates = session.execute_write(write_eu,statement_eu_mem, eu_members)\n",
    "    print(\"Countries member of EU added:\",total_updates) # Kosovo and Taiwan\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import GeoAreas to Neo4j from Eurostat (NUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nomenclature of Territorial Units for Statistics or NUTS is a geocode standard for referencing the subdivisions of countries for statistical purposes. For each EU member country, a hierarchy of three NUTS levels is established by **Eurostat** in agreement with each member state. In the folder Data, there is a excel file(1.GeoArea_EU_NUTS.xlsx), containing information about the NUTS levels, that we are going to import into our graph. Data were collected from : https://ec.europa.eu/eurostat/web/nuts/overview (version NUTS 2021 classification)\n",
    "\n",
    "Before importing the data into neo4j, the excel file is transformed into the desired format. The data transformation follows the below processing:\n",
    "\n",
    "- Drop unnecessary columns and rows (Data cleaning)\n",
    "- Unpivot pandas dataframe\n",
    "- Add extra column about the higher NUTS level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(word,n):\n",
    "    chunks = [word[i:i+n] for i in range(0, len(word), n)]\n",
    "    return chunks\n",
    "\n",
    "# Read excel file\n",
    "df_nuts = pd.read_excel('Data/1.GeoArea_EU_NUTS.xlsx',sheet_name='NUTS2021')\n",
    "\n",
    "# Drop unnecessary columns & rows\n",
    "df_nuts.drop(['Country order', 'Region order'], axis=1, inplace=True)\n",
    "df_nuts.drop(df_nuts.loc[df_nuts['Code 2021'].str.endswith('Z')].index, inplace=True)\n",
    "\n",
    "# Create new column Length\n",
    "df_nuts[\"Length\"]= df_nuts[\"Code 2021\"].str.len()\n",
    "\n",
    "# Unpivot dataframe\n",
    "df = df_nuts.melt(id_vars=['Code 2021','NUTS level','Length'],var_name='Level', value_name='Name')\n",
    "## Drop NA & reset index\n",
    "df = df.dropna(subset=['Name'])\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Add new Column Country\n",
    "df['SuperClass'] = df.apply(lambda x: split(x['Code 2021'],x['Length']-1)[0], axis=1) \n",
    "# Drop countries\n",
    "df.drop(df.loc[df['Length'] == 2].index, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuts_creation(df1,name_of_rel):\n",
    "      \n",
    "    records, summary, keys = driver.execute_query(\"\"\"\n",
    "        MATCH (ga:GeoArea)\n",
    "        WHERE ga.EUcode IS NOT NULL\n",
    "        RETURN COLLECT(ga.EUcode) as geocodes\n",
    "        \"\"\",routing_=\"r\",database_=database_name)\n",
    "    available_neo4j_geocodes = records[0]['geocodes']\n",
    "\n",
    "    # Keep only the rows containing values of countries existing in the neo4j LPG model.\n",
    "    df1 = df1.loc[df1['SuperClass'].isin(available_neo4j_geocodes)]\n",
    "    \n",
    "    if name_of_rel == 'HAS_NUTS1':\n",
    "        statement_nuts = \"\"\"\n",
    "        UNWIND $parameters as row\n",
    "        MATCH (super_class:GeoArea)\n",
    "        WHERE row.super_code = super_class.EUcode\n",
    "        MERGE (super_class)-[:HAS_NUTS1]->(ga:GeoArea{name:row.nuts_name,EUcode:row.nuts_code})\n",
    "        \"\"\"    \n",
    "    if name_of_rel == 'HAS_NUTS2':\n",
    "        statement_nuts = \"\"\"\n",
    "        UNWIND $parameters as row\n",
    "        MATCH (super_class:GeoArea)\n",
    "        WHERE row.super_code = super_class.EUcode\n",
    "        MERGE (super_class)-[:HAS_NUTS2]->(ga:GeoArea{name:row.nuts_name,EUcode:row.nuts_code})\n",
    "        \"\"\" \n",
    "    if name_of_rel == 'HAS_NUTS3':\n",
    "        statement_nuts = \"\"\"\n",
    "        UNWIND $parameters as row\n",
    "        MATCH (super_class:GeoArea)\n",
    "        WHERE row.super_code = super_class.EUcode\n",
    "        MERGE (super_class)-[:HAS_NUTS3]->(ga:GeoArea{name:row.nuts_name,EUcode:row.nuts_code})\n",
    "        \"\"\" \n",
    "        \n",
    "    # Begin a new auto-commit GraphTransaction.\n",
    "    with driver.session(database=database_name) as session:\n",
    "        params=[]\n",
    "        for index, row in df1.iterrows():\n",
    "            params_dict = {\n",
    "                'super_code': str(row['SuperClass']), \n",
    "                'nuts_name': str(row['Name']).strip(),\n",
    "                'nuts_code': str(row['Code 2021'])\n",
    "            }\n",
    "            params.append(params_dict)\n",
    "        st = time.time()\n",
    "        session.execute_write(write_batch, params_list = params,statement = statement_nuts)\n",
    "        elapsed_time = time.time() - st\n",
    "        print('{} observations: Done! ({} minutes)'.format(len(params), elapsed_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 observations: Done! (0.0013754129409790038 minutes)\n",
      "334 observations: Done! (0.0018534143765767416 minutes)\n",
      "1514 observations: Done! (0.00373154878616333 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Add sequentially each NUTS level \n",
    "nuts_creation(df1 = df.loc[df.Length==3],name_of_rel= 'HAS_NUTS1')\n",
    "nuts_creation(df1 = df.loc[df.Length==4],name_of_rel= 'HAS_NUTS2')\n",
    "nuts_creation(df1 = df.loc[df.Length==5],name_of_rel= 'HAS_NUTS3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing the following cypher query, we check the size of imported data (in this way duplication of data can be avoided.)\n",
    "> Check cypher query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUTS1: 125\n",
      "NUTS2: 334\n",
      "NUTS3: 1514\n"
     ]
    }
   ],
   "source": [
    "def count_nuts(tx,statement):\n",
    "    result = tx.run(statement)\n",
    "    data = result.data()[0]\n",
    "    return data['nuts1'],data['nuts2'], data['nuts3']\n",
    "\n",
    "statement_nuts = \"\"\" \n",
    "MATCH (ga:GeoArea)-[:HAS_NUTS1]->(r)-[:HAS_NUTS2]->(sb)-[:HAS_NUTS3]->(a)\n",
    "RETURN COUNT (DISTINCT r) as nuts1 ,COUNT(DISTINCT sb) as nuts2 ,count(DISTINCT a) as nuts3\n",
    "\"\"\" \n",
    "with driver.session(database=database_name) as session:\n",
    "    nuts1,nuts2,nuts3 = session.execute_read(count_nuts, statement_nuts)\n",
    "    print('NUTS1:', nuts1)\n",
    "    print('NUTS2:', nuts2)\n",
    "    print('NUTS3:', nuts3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have imported the geoAreas, we add a second label, indicating if this geoArea is a region,subregion,etc.\n",
    "\n",
    "> Set second GeoArea label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(tx,statement):\n",
    "    tx.run(statement)\n",
    "    return \n",
    "\n",
    "statement_labels = [\n",
    "\"\"\" MATCH (g1:GeoArea)-[r:HAS_AREA]->(g2:GeoArea)\n",
    "SET g2:Area\"\"\",\n",
    "\"\"\"MATCH (g1:GeoArea)-[r:HAS_SUBREGION]->(g2:GeoArea)\n",
    "SET g2:SubRegion\"\"\",\n",
    "\"\"\"MATCH (g1:GeoArea)-[r:HAS_REGION]->(g2:GeoArea)\n",
    "SET g2:Region\"\"\",\n",
    "\"\"\"MATCH (g1:GeoArea)-[r:HAS_NUTS3]->(g2:GeoArea)\n",
    "SET g2:NUTS3\"\"\",\n",
    "\"\"\"MATCH (g1:GeoArea)-[r:HAS_NUTS2]->(g2:GeoArea)\n",
    "SET g2:NUTS2\"\"\",\n",
    "\"\"\"MATCH (g1:GeoArea)-[r:HAS_NUTS1]->(g2:GeoArea)\n",
    "SET g2:NUTS1\"\"\"\n",
    "] \n",
    "\n",
    "with driver.session(database=database_name) as session:\n",
    "    for statement_label in statement_labels:\n",
    "        session.execute_write(add_labels, statement_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import Eurostat Typology of NUTS3 Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraints\n",
    "\n",
    "Each NUTS3 is classified according to a NUTS Typology provided by Eurostat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_constraint = \"\"\" \n",
    "CREATE CONSTRAINT typology IF NOT EXISTS FOR ( top:Typology ) REQUIRE (top.categoryCode, top.categoryLabel) IS NODE KEY\n",
    "\"\"\" \n",
    "\n",
    "with driver.session(database=database_name) as session:\n",
    "    session.execute_write(create_constraint, statement_constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Eurostat typology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read csv \n",
    "cols = ['UrbanRural','Metropolitan','Coastal',\n",
    "        'Mountain','Border','Island','UrbanRuralRemoteness']\n",
    "df_typology = pd.read_excel('Data/1.GeoArea_EU_NUTS.xlsx',sheet_name = cols )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_batch(tx,statement, params_list):\n",
    "    tx.run(statement, parameters={\"parameters\": params_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1514 observations: Done! (0.012930019696553548 minutes)\n",
      "1514 observations: Done! (0.008372116088867187 minutes)\n",
      "1514 observations: Done! (0.0077974836031595865 minutes)\n",
      "1512 observations: Done! (0.00818260113398234 minutes)\n",
      "1512 observations: Done! (0.007384232680002848 minutes)\n",
      "73 observations: Done! (0.001057298978169759 minutes)\n",
      "1387 observations: Done! (0.006750647226969401 minutes)\n"
     ]
    }
   ],
   "source": [
    "statement_top = \"\"\"\n",
    "    UNWIND $parameters as row\n",
    "    MATCH (ga:NUTS3{EUcode:row.nuts_code})\n",
    "    MERGE (top:Typology{categoryCode:row.category,categoryLabel:row.label})\n",
    "    MERGE (ga)-[:HAS_TYPOLOGY]-> (top)\n",
    "    WITH top, row\n",
    "    CALL apoc.create.addLabels( top, [ row.name] )\n",
    "    YIELD node\n",
    "    RETURN node\n",
    "    \"\"\"    \n",
    "\n",
    "for col in cols:\n",
    "    \n",
    "    df_col = df_typology[col] \n",
    "    \n",
    "    with driver.session(database=database_name) as session:\n",
    "        params=[]\n",
    "        for index, row in df_col.iterrows():        \n",
    "            params_dict = {\n",
    "                'nuts_code':str(row['Nuts']),\n",
    "                'category':str(row['Category']).title(),\n",
    "                'label':str(row['Label']).title(),\n",
    "                'name':str(col)\n",
    "            }\n",
    "            params.append(params_dict)\n",
    "        st = time.time()\n",
    "        session.execute_write(write_batch, params_list = params,statement = statement_top)\n",
    "        elapsed_time = time.time() - st\n",
    "        print('{} observations: Done! ({} minutes)'.format(len(params), elapsed_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check cypher query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total typologies: 9026 , expected: 20\n",
      "Total urban_rural nuts3: 1514 , expected: 1514\n"
     ]
    }
   ],
   "source": [
    "def count_typology(tx,statement):\n",
    "    result = tx.run(statement)\n",
    "    data = result.data()[0]\n",
    "    return data['total']\n",
    "\n",
    "statement_typo= \"\"\" \n",
    "MATCH (n:Typology) RETURN count(distinct n) as total\n",
    "\"\"\" \n",
    "statement_typo_rel = \"\"\"\n",
    "MATCH (ga:NUTS3)-[r:HAS_TYPOLOGY]->(:UrbanRural) RETURN count(r) as total\n",
    "\"\"\"\n",
    "\n",
    "unique_labels = 0 \n",
    "for sheet_name, df in df_typology.items():\n",
    "    unique_labels += len(df['Label'].unique())\n",
    "\n",
    "df_NUTS3 = pd.read_excel('Data/1.GeoArea_EU_NUTS.xlsx',sheet_name = 'NUTS2021') \n",
    "nuts3_codes = df_NUTS3[df_NUTS3['NUTS level'] == 3]['Code 2021'].dropna().unique().tolist()\n",
    "df_urbanrural = pd.read_excel('Data/1.GeoArea_EU_NUTS.xlsx', sheet_name='UrbanRural')\n",
    "nuts3_labels = df_urbanrural[df_urbanrural['Nuts'].isin(nuts3_codes)]['Label']\n",
    "labels3_list = nuts3_labels.dropna().tolist()\n",
    "\n",
    "with driver.session(database=database_name) as session:\n",
    "    total = session.execute_read(count_typology, statement_typo)\n",
    "    total_rel = session.execute_read(count_typology, statement_typo_rel)\n",
    "    print('Total typologies:', total, ', expected:', unique_labels) \n",
    "    print('Total urban_rural nuts3:', total_rel, ', expected:', len(labels3_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Eurostat LAU,Cities,FUA, Degree of Urbanization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To meet the demand for statistics at a local level, Eurostat maintains a system of Local Administrative Units (LAUs) compatible with NUTS. These LAUs are the building blocks of the NUTS, and comprise the municipalities and communes of the European Union. As defined by Eurostat:\n",
    "\n",
    "* A City is a local administrative unit (LAU) where the majority of the population lives in an urban centre of at least 50 000 inhabitants.\n",
    "* The Functional Urban Area (FUA) consists of a city and its commuting zone.\n",
    "* The Degree of urbanization (DEGURBA) is a classification that indicates the character of an area: Cities (densely populated areas) - Towns and suburbs (intermediate density areas) - Rural areas (thinly populated areas)\n",
    "\n",
    "![SustainGraph-Local_level__2_](https://gitlab.com/netmode/sustaingraph/-/wikis/uploads/c726fa43617fa6b5ef9046da9303e653/SustainGraph-Local_level.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree of urbanization\n",
    "# https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Glossary:Degree_of_urbanisation \n",
    "degurb = {1:'Cities',\n",
    "          2:'TownsAndSuburbs',\n",
    "          3:'RuralAreas'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some LAUs may not belong to neither a City nor a FUA\n",
    "statement_local = \"\"\"\n",
    "    UNWIND $parameters as row\n",
    "    WITH row \n",
    "    MATCH (nuts3:NUTS3{EUcode:row.nuts_code})\n",
    "    MERGE (lau:LAU{EUcode:row.lau_code,name:row.lau_name})\n",
    "    ON CREATE\n",
    "        SET lau:GeoArea\n",
    "    MERGE (nuts3)-[:HAS_LAU]-> (lau)\n",
    "    FOREACH(_ IN CASE WHEN row.deg_code IS NOT NULL THEN [1] ELSE [] END | \n",
    "    MERGE (degurba:UrbanizationDegree{categoryCode:row.deg_code,categoryLabel:row.deg_name})\n",
    "    ON CREATE\n",
    "        SET degurba:Typology\n",
    "    MERGE (lau) -[:HAS_DEGURBA]-> (degurba)\n",
    "    )\n",
    "    FOREACH(_ IN CASE WHEN row.city_code IS NOT NULL THEN [1] ELSE [] END | \n",
    "    MERGE (city:City{EUcode:row.city_code,name:row.city_name})\n",
    "       ON CREATE\n",
    "         SET city:GeoArea\n",
    "    MERGE (city) -[:CONSISTS_OF]-> (lau)\n",
    "    )\n",
    "    FOREACH(_ IN CASE WHEN row.fua_code IS  NOT NULL THEN [1] ELSE [] END | \n",
    "    MERGE (fua:FUA{EUcode:row.fua_code,name:row.fua_name})\n",
    "    ON CREATE\n",
    "        SET fua:GeoArea\n",
    "    MERGE (fua) -[:CONSISTS_OF]-> (lau)\n",
    "    )\n",
    "    FOREACH(_ IN CASE WHEN row.fua_code IS  NOT NULL AND row.city_code IS NOT NULL THEN [1] ELSE [] END | \n",
    "    MERGE(fua:FUA{EUcode:row.fua_code})\n",
    "    MERGE(city:City{EUcode:row.city_code})\n",
    "    MERGE (fua) -[:CONSISTS_OF]-> (city)\n",
    "    )\n",
    "    \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581 observations: Done! (0.012862447897593181 minutes)\n",
      "BG\n",
      "265 observations: Done! (0.0030786673227945964 minutes)\n",
      "CZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n",
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6258 observations: Done! (0.12043026685714722 minutes)\n",
      "DK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 observations: Done! (0.013086044788360595 minutes)\n",
      "DE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11002 observations: Done! (0.5505887667338053 minutes)\n",
      "IE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 observations: Done! (0.01391056776046753 minutes)\n",
      "EE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 observations: Done! (0.007532250881195068 minutes)\n",
      "EL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6137 observations: Done! (0.48239006598790485 minutes)\n",
      "ES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8131 observations: Done! (0.9087620337804159 minutes)\n",
      "FR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/9fyjfdcd1ns5b9syk2n5lktm0000gn/T/ipykernel_74110/1489562237.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n"
     ]
    }
   ],
   "source": [
    "df_lau_all = pd.read_excel('Data/1.GeoArea_EU-27-LAU-2021-NUTS-2021.xlsx',sheet_name=None,dtype='object')\n",
    "for name, sheet in df_lau_all.items():\n",
    "    if(name == 'Overview POP_2021' ):\n",
    "        continue\n",
    "    print(name)\n",
    "    #REPLACE 9 DEGREE of urbanization with None\n",
    "    sheet['DEGURBA'] = sheet['DEGURBA'] .replace([9], np.nan)\n",
    "    sheet['DEGURBA_NAME'] = sheet['DEGURBA'].map(degurb)\n",
    "    #--------------------------fix Eurostat errors----------------------------------------\n",
    "    if(name == 'EL'):\n",
    "        sheet['CITY_NAME'] = sheet['CITY_NAME'].replace('Narva', 'Athens (Greater City)')\n",
    "    elif (name =='ES'):\n",
    "        sheet['FUA_NAME'] = sheet['FUA_NAME'].replace('Ponteverda', 'Pontevedra')\n",
    "    \n",
    "    df_lau = sheet[['NUTS 3 CODE','LAU CODE', 'LAU NAME LATIN', 'DEGURBA','DEGURBA_NAME','CITY_ID','CITY_NAME','FUA_ID','FUA_NAME']]\n",
    "    # -------------------------import data-------------------------------------------\n",
    "    with driver.session(database=database_name) as session:\n",
    "        params=[]\n",
    "        for index, row in df_lau.iterrows(): \n",
    "            # country code added in front of lau code to make it unique for each country        \n",
    "            params_dict = {\n",
    "                'nuts_code':str(row['NUTS 3 CODE']),\n",
    "                'lau_code':f\"{name}_{(str(row['LAU CODE']))}\",\n",
    "                'lau_name':str(row['LAU NAME LATIN']).strip(),\n",
    "                'deg_code':str(int(row['DEGURBA'])) if not pd.isna(row['DEGURBA']) else None,  # Keep it as None if NaN,\n",
    "                'deg_name':str(row['DEGURBA_NAME']),\n",
    "                'city_code':str(row['CITY_ID']) if not pd.isna(row['CITY_ID']) else None,  # Keep it as None if NaN, ,\n",
    "                'city_name':str(row['CITY_NAME']).strip() if not pd.isna(row['CITY_NAME']) else None,  # Keep it as None if NaN, ,\n",
    "                'fua_code':str(row['FUA_ID']) if not pd.isna(row['FUA_ID']) else None,  # Keep it as None if NaN, ,\n",
    "                'fua_name':str(row['FUA_NAME']).strip() if not pd.isna(row['FUA_NAME']) else None,  # Keep it as None if NaN, ,\n",
    "            }\n",
    "            params.append(params_dict)\n",
    "        \n",
    "        st = time.time()\n",
    "        session.execute_write(write_batch, params_list = params,statement = statement_local)\n",
    "        elapsed_time = time.time() - st\n",
    "        print('{} observations: Done! ({} minutes)'.format(len(params), elapsed_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with the number of LAUs,FUAs and Cities per country to compare with the cypher check query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lau_all = pd.read_excel('Data/1.GeoArea_EU-27-LAU-2021-NUTS-2021.xlsx',sheet_name=None,dtype='object')\n",
    "df_check = []\n",
    "columns = ['country','totalLAU','totalCities','totalFUA']\n",
    "for name, sheet in df_lau_all.items():\n",
    "    if(name == 'Overview POP_2021' ):\n",
    "        continue\n",
    "    df_check.append([name,sheet['LAU CODE'].nunique(),sheet['CITY_ID'].nunique(),sheet['FUA_ID'].nunique()])\n",
    "df = pd.DataFrame(df_check, columns=columns)\n",
    "df= df.sort_values('country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>totalLAU</th>\n",
       "      <th>totalCities</th>\n",
       "      <th>totalFUA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AT</td>\n",
       "      <td>2095</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BE</td>\n",
       "      <td>581</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG</td>\n",
       "      <td>265</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CY</td>\n",
       "      <td>615</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CZ</td>\n",
       "      <td>6258</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>11002</td>\n",
       "      <td>127</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DK</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EE</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EL</td>\n",
       "      <td>6137</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ES</td>\n",
       "      <td>8131</td>\n",
       "      <td>98</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FI</td>\n",
       "      <td>310</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FR</td>\n",
       "      <td>34966</td>\n",
       "      <td>77</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HR</td>\n",
       "      <td>556</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HU</td>\n",
       "      <td>3155</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IE</td>\n",
       "      <td>166</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IT</td>\n",
       "      <td>7903</td>\n",
       "      <td>87</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LT</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LU</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LV</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MT</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NL</td>\n",
       "      <td>355</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PL</td>\n",
       "      <td>2477</td>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PT</td>\n",
       "      <td>3092</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RO</td>\n",
       "      <td>3181</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SE</td>\n",
       "      <td>290</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SI</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SK</td>\n",
       "      <td>2927</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  totalLAU  totalCities  totalFUA\n",
       "19      AT      2095            6         6\n",
       "0       BE       581           15        14\n",
       "1       BG       265           18        17\n",
       "12      CY       615            3         3\n",
       "2       CZ      6258           18        15\n",
       "4       DE     11002          127        98\n",
       "3       DK        99            4         4\n",
       "6       EE        79            3         3\n",
       "7       EL      6137           14        14\n",
       "8       ES      8131           98        81\n",
       "25      FI       310            9         7\n",
       "9       FR     34966           77        70\n",
       "10      HR       556            7         7\n",
       "16      HU      3155           19        19\n",
       "5       IE       166            5         5\n",
       "11      IT      7903           87        83\n",
       "14      LT        60            6         6\n",
       "15      LU       102            1         1\n",
       "13      LV       119            4         4\n",
       "17      MT        68            1         1\n",
       "18      NL       355           47        35\n",
       "20      PL      2477           68        58\n",
       "21      PT      3092           16        12\n",
       "22      RO      3181           35        35\n",
       "26      SE       290           14        12\n",
       "23      SI       212            2         2\n",
       "24      SK      2927            8         8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Note: Greece (EL) has one less LAU code in the cypher query check than in the table above because Aghio Oros LAU has not matched any NUTS3 code in the SustainGraph therefore is not imported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordsLAU, summary, keys = driver.execute_query(\"\"\"\n",
    "    MATCH (l:LAU)\n",
    "    RETURN LEFT(l.EUcode, 2) AS country_code, count(l) AS LAUCount\n",
    "    ORDER BY country_code\n",
    "    \"\"\",routing_=\"r\",database_=database_name)\n",
    "countries = []\n",
    "laus = []\n",
    "for record in recordsLAU:\n",
    "    countries.append(record['country_code'])\n",
    "    laus.append(record['LAUCount'])\n",
    "\n",
    "recordsCity, summary, keys = driver.execute_query(\"\"\"\n",
    "    MATCH (c:City)\n",
    "    RETURN LEFT(c.EUcode, 2) AS country_code, count(c) AS CityCount\n",
    "    ORDER BY country_code\n",
    "    \"\"\",routing_=\"r\",database_=database_name)\n",
    "cities = []\n",
    "for record in recordsCity:\n",
    "    cities.append(record['CityCount'])\n",
    "\n",
    "recordsFUA, summary, keys = driver.execute_query(\"\"\"\n",
    "    MATCH (f:FUA)\n",
    "    RETURN LEFT(f.EUcode, 2) AS country_code, count(f) AS FUACount\n",
    "    ORDER BY country_code\n",
    "    \"\"\",routing_=\"r\",database_=database_name)\n",
    "fua = []\n",
    "for record in recordsFUA:\n",
    "    fua.append(record['FUACount'])\n",
    "\n",
    "check_df = pd.DataFrame(\n",
    "    {'country': countries,\n",
    "     'totalLAU': laus,\n",
    "     'totalCities': cities,\n",
    "     'totalFUA':fua\n",
    "    })\n",
    "merged_df = pd.merge(df, check_df, on='country', suffixes=('_1', '_2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>totalLAU_1</th>\n",
       "      <th>totalCities_1</th>\n",
       "      <th>totalFUA_1</th>\n",
       "      <th>totalLAU_2</th>\n",
       "      <th>totalCities_2</th>\n",
       "      <th>totalFUA_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT</td>\n",
       "      <td>2095</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2095</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BE</td>\n",
       "      <td>581</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>581</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BG</td>\n",
       "      <td>265</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>265</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CY</td>\n",
       "      <td>615</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>615</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CZ</td>\n",
       "      <td>6258</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>6258</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DE</td>\n",
       "      <td>11002</td>\n",
       "      <td>127</td>\n",
       "      <td>98</td>\n",
       "      <td>11002</td>\n",
       "      <td>127</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DK</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EE</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EL</td>\n",
       "      <td>6137</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>6136</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ES</td>\n",
       "      <td>8131</td>\n",
       "      <td>98</td>\n",
       "      <td>81</td>\n",
       "      <td>8131</td>\n",
       "      <td>98</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FI</td>\n",
       "      <td>310</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>310</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FR</td>\n",
       "      <td>34966</td>\n",
       "      <td>77</td>\n",
       "      <td>70</td>\n",
       "      <td>34966</td>\n",
       "      <td>77</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HR</td>\n",
       "      <td>556</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>556</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HU</td>\n",
       "      <td>3155</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3155</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IE</td>\n",
       "      <td>166</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IT</td>\n",
       "      <td>7903</td>\n",
       "      <td>87</td>\n",
       "      <td>83</td>\n",
       "      <td>7903</td>\n",
       "      <td>87</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LT</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LU</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LV</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MT</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NL</td>\n",
       "      <td>355</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>355</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PL</td>\n",
       "      <td>2477</td>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "      <td>2477</td>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PT</td>\n",
       "      <td>3092</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>3092</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RO</td>\n",
       "      <td>3181</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>3181</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SE</td>\n",
       "      <td>290</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>290</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SI</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SK</td>\n",
       "      <td>2927</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2927</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  totalLAU_1  totalCities_1  totalFUA_1  totalLAU_2  totalCities_2  \\\n",
       "0       AT        2095              6           6        2095              6   \n",
       "1       BE         581             15          14         581             15   \n",
       "2       BG         265             18          17         265             18   \n",
       "3       CY         615              3           3         615              3   \n",
       "4       CZ        6258             18          15        6258             18   \n",
       "5       DE       11002            127          98       11002            127   \n",
       "6       DK          99              4           4          99              4   \n",
       "7       EE          79              3           3          79              3   \n",
       "8       EL        6137             14          14        6136             14   \n",
       "9       ES        8131             98          81        8131             98   \n",
       "10      FI         310              9           7         310              9   \n",
       "11      FR       34966             77          70       34966             77   \n",
       "12      HR         556              7           7         556              7   \n",
       "13      HU        3155             19          19        3155             19   \n",
       "14      IE         166              5           5         166              5   \n",
       "15      IT        7903             87          83        7903             87   \n",
       "16      LT          60              6           6          60              6   \n",
       "17      LU         102              1           1         102              1   \n",
       "18      LV         119              4           4         119              4   \n",
       "19      MT          68              1           1          68              1   \n",
       "20      NL         355             47          35         355             47   \n",
       "21      PL        2477             68          58        2477             68   \n",
       "22      PT        3092             16          12        3092             16   \n",
       "23      RO        3181             35          35        3181             35   \n",
       "24      SE         290             14          12         290             14   \n",
       "25      SI         212              2           2         212              2   \n",
       "26      SK        2927              8           8        2927              8   \n",
       "\n",
       "    totalFUA_2  \n",
       "0            6  \n",
       "1           14  \n",
       "2           17  \n",
       "3            3  \n",
       "4           15  \n",
       "5           98  \n",
       "6            4  \n",
       "7            3  \n",
       "8           14  \n",
       "9           81  \n",
       "10           7  \n",
       "11          70  \n",
       "12           7  \n",
       "13          19  \n",
       "14           5  \n",
       "15          83  \n",
       "16           6  \n",
       "17           1  \n",
       "18           4  \n",
       "19           1  \n",
       "20          35  \n",
       "21          58  \n",
       "22          12  \n",
       "23          35  \n",
       "24          12  \n",
       "25           2  \n",
       "26           8  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total relationships created:  620\n"
     ]
    }
   ],
   "source": [
    "# Connect FUA to their countries\n",
    "def create_rel(tx, statement):\n",
    "    records = tx.run(statement)\n",
    "    return records.data()[0]['total_fua']\n",
    "\n",
    "statement_country_fua = \"\"\" \n",
    "MATCH (a:Area)-[:HAS_NUTS1]-(:NUTS1)-[:HAS_NUTS2]-(:NUTS2)-[HAS_NUTS3]-(:NUTS3)-[:HAS_LAU]-(:LAU)-[:CONSISTS_OF]-(f:FUA)\n",
    "MERGE (a)-[h:HAS_FUA]->(f)\n",
    "RETURN COUNT(DISTINCT h) as total_fua\n",
    "\"\"\"\n",
    "\n",
    "with driver.session(database=database_name) as session:\n",
    "    total_fua = session.execute_write(create_rel,statement_country_fua)\n",
    "    print('Total relationships created: ', total_fua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Eurostat PostalCodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [mapping of postal codes with NUTS3 regions](https://gisco-services.ec.europa.eu/tercet/flat-files) of Eurostat, we introduce the zip codes only for Greece in the SustainGraph. In the folder Data/1.GeoArea_EU_PostalCodes, there is a csv file for Greece, containing information about the mapping of NUTS3 regions with the postal codes, that we are going to import into our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! Import only Greece data due to large number of postal codes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_post = \"\"\"\n",
    "    UNWIND $parameters as row\n",
    "    WITH row \n",
    "    MATCH (ga:NUTS3{EUcode:toString(row.nuts_code)})\n",
    "    MERGE (post:PostalCode{EUcode:toString(row.postal_code)})\n",
    "    ON CREATE\n",
    "        SET post:GeoArea\n",
    "    MERGE (ga)-[:HAS_POSTAL_CODE]-> (post)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eleftheria\\AppData\\Local\\Temp\\ipykernel_6080\\3306041594.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory (where the .ipynb file is located)\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the path to the 'Data' folder\n",
    "oecd_data_path = os.path.join(current_directory, 'Data/1.GeoArea_EU_PostalCodes')\n",
    "\n",
    "# List all CSV files in the 'OECD_Data' folder\n",
    "csv_files = [f for f in os.listdir(oecd_data_path) if f.endswith('.csv')]\n",
    "\n",
    "dict_of_dfs = {}\n",
    "batch_size=5000\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    csv_path = os.path.join(oecd_data_path, csv_file)\n",
    "    # Example: Read CSV content using pandas\n",
    "\n",
    "    # Original df\n",
    "    df = pd.read_csv(csv_path.replace('\\\\','/'),delimiter=';')\n",
    "    \n",
    "    # delete ''\n",
    "    df = df.applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x)\n",
    "    \n",
    "    dict_of_dfs[csv_file.split('_')[1]] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country EL data import\n",
      "Expected Length of data:  1041\n",
      "1041 observations: Done! (0.012144529819488525 minutes)\n"
     ]
    }
   ],
   "source": [
    "batch_size=5000\n",
    "for key,df in dict_of_dfs.items():\n",
    "    print('Country {} data import'.format(key))\n",
    "    # Begin a new auto-commit GraphTransaction.\n",
    "    params=[]\n",
    "    batch_i = 1\n",
    "    with driver.session(database=database_name) as session:\n",
    "        for index, row in df.iterrows():\n",
    "            # get the start time\n",
    "            st = time.time()\n",
    "            params_dict = {\n",
    "                'nuts_code': str(row['NUTS3']), \n",
    "                'postal_code': str(row['CODE']),\n",
    "            }\n",
    "            params.append(params_dict)\n",
    "            if index % batch_size == 0 and index > 0:\n",
    "                st = time.time()\n",
    "                session.execute_write(write_batch, params_list = params,statement = statement_post)\n",
    "                # get the end time\n",
    "                et = time.time()\n",
    "                # get the execution time\n",
    "                elapsed_time = et - st            \n",
    "                print('Batch {} with {} observations : Done! ({} minutes)'.format(batch_i,len(params),elapsed_time/60))\n",
    "                params = []            \n",
    "                batch_i +=1\n",
    "        if params:\n",
    "            st = time.time()  # Record start time for the last batch\n",
    "            session.execute_write(write_batch, params_list = params,statement = statement_post)\n",
    "            et = time.time()\n",
    "            elapsed_time = time.time() - st\n",
    "            print('Expected Length of data: ', len(df))\n",
    "            print('{} observations: Done! ({} minutes)'.format(len(params), elapsed_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check cypher query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notios Tomeas Athinon with 30  postal codes\n",
      "Kalymnos, Karpathos, Kasos, Kos, Rodos with 21  postal codes\n",
      "Evros with 15  postal codes\n",
      "Kilkis with 7  postal codes\n",
      "Kastoria with 12  postal codes\n",
      "Magnisia, Sporades with 22  postal codes\n",
      "Achaia with 31  postal codes\n",
      "Argolida, Arkadia with 28  postal codes\n",
      "Voreios Tomeas Athinon with 38  postal codes\n",
      "Dytikos Tomeas Athinon with 20  postal codes\n",
      "Kentrikos Tomeas Athinon with 115  postal codes\n",
      "Anatoliki Attiki with 36  postal codes\n",
      "Dytiki Attiki with 13  postal codes\n",
      "Peiraias, Nisoi with 41  postal codes\n",
      "Lesvos, Limnos with 13  postal codes\n",
      "Ikaria, Samos with 9  postal codes\n",
      "Chios with 6  postal codes\n",
      "Andros, Thira, Kea, Milos, Mykonos, Naxos, Paros,  Syros, Tinos with 23  postal codes\n",
      "Irakleio with 29  postal codes\n",
      "Lasithi with 10  postal codes\n",
      "Rethymni with 9  postal codes\n",
      "Chania with 22  postal codes\n",
      "Xanthi with 6  postal codes\n",
      "Rodopi with 4  postal codes\n",
      "Drama with 8  postal codes\n",
      "Thasos, Kavala with 16  postal codes\n",
      "Imathia with 8  postal codes\n",
      "Thessaloniki with 84  postal codes\n",
      "Pella with 9  postal codes\n",
      "Pieria with 9  postal codes\n",
      "Serres with 21  postal codes\n",
      "Chalkidiki with 17  postal codes\n",
      "Grevena, Kozani with 15  postal codes\n",
      "Florina with 10  postal codes\n",
      "Arta, Preveza with 15  postal codes\n",
      "Thesprotia with 4  postal codes\n",
      "Ioannina with 20  postal codes\n",
      "Karditsa, Trikala with 23  postal codes\n",
      "Larisa with 22  postal codes\n",
      "Zakynthos with 4  postal codes\n",
      "Kerkyra with 6  postal codes\n",
      "Ithaki, Kefallinia with 7  postal codes\n",
      "Lefkada with 3  postal codes\n",
      "Aitoloakarnania with 30  postal codes\n",
      "Ileia with 21  postal codes\n",
      "Voiotia with 15  postal codes\n",
      "Evvoia with 23  postal codes\n",
      "Evrytania with 7  postal codes\n",
      "Fthiotida with 18  postal codes\n",
      "Fokida with 10  postal codes\n",
      "Korinthia with 18  postal codes\n",
      "Lakonia, Messinia with 38  postal codes\n"
     ]
    }
   ],
   "source": [
    "records, summary, keys = driver.execute_query(\"\"\"\n",
    "   MATCH (ga:NUTS3)-[:HAS_POSTAL_CODE]->(n:PostalCode) RETURN ga.name,COUNT(n) as codes\n",
    "    \"\"\",routing_=\"r\",database_=database_name)\n",
    "for record in records:\n",
    "    print(record['ga.name'], 'with', record['codes'],' postal codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_postal(tx,statement):\n",
    "    tx.run(statement)\n",
    "    return \n",
    "\n",
    "statement_postal= \"\"\"\n",
    "MATCH (n3:NUTS3 {EUcode: 'EL306'}) \n",
    "MERGE (n3)-[:HAS_POSTAL_CODE]->(z:PostalCode{EUcode:'13345'}) \n",
    "ON CREATE SET z:GeoArea \n",
    " \"\"\"\n",
    "\n",
    "with driver.session(database=database_name) as session:\n",
    "    session.execute_write(add_postal, statement_postal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_postal_area(tx,statement):\n",
    "    tx.run(statement)\n",
    "    return \n",
    "\n",
    "statement_postal_area= \"\"\"\n",
    "MATCH (n3:NUTS3)-[:HAS_POSTAL_CODE]->(post:PostalCode)\n",
    "MATCH (ga:Area)--(n1:NUTS1)--(n2:NUTS2)--(n3)\n",
    "MERGE (ga)-[r:HAS_POSTAL_CODE]->(post)\n",
    "RETURN count(distinct r)\n",
    " \"\"\"\n",
    "\n",
    "with driver.session(database=database_name) as session:\n",
    "    session.execute_write(add_postal, statement_postal_area)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
